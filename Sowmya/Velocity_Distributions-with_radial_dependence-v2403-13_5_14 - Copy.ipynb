{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba744b50-7c32-4773-84f7-c656f27ba88a",
   "metadata": {},
   "source": [
    "# Velocity Distributions - Generate scale-independent fits\n",
    "\n",
    "2 sigmas and lambda - $10^{13.5-14}$ $M_\\odot$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bcdf28-8e2e-462e-bd82-11af6e69eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure,show\n",
    "from swiftsimio import load\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import matplotlib.mlab as mlab\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import root, minimize\n",
    "from lmfit.models import LorentzianModel, VoigtModel, GaussianModel,LognormalModel\n",
    "from scipy.integrate import fixed_quad, quad, dblquad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85a766-d8b6-4bb2-8d7b-61127a493c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_hydro= \"/net/hypernova/data2/FLAMINGO/L1000N1800/HYDRO_FIDUCIAL/SOAP-HBT/halo_properties_0077.hdf5\"\n",
    "with h5py.File(path_hydro, \"r\") as handle:\n",
    "    TotalMass= handle[\"ExclusiveSphere/100kpc/TotalMass\"][:]\n",
    "    StellarMass = handle[\"ExclusiveSphere/100kpc/StellarMass\"][:]\n",
    "    COMstellarvelocity=handle[\"ExclusiveSphere/100kpc/StellarCentreOfMassVelocity\"][:]\n",
    "    COMvelocity=handle[\"ExclusiveSphere/100kpc/CentreOfMassVelocity\"][:]\n",
    "    Trackid=handle[\"InputHalos/HBTplus/TrackId\"][:]\n",
    "    HOSTFOFID=handle[\"InputHalos/HBTplus/HostFOFId\"][:]\n",
    "    HaloCatalogueIndex=handle[\"InputHalos/HaloCatalogueIndex\"][:]\n",
    "    HOSTHALOINDEX=handle[\"SOAP/HostHaloIndex\"][:]\n",
    "    FOFMass=handle[\"InputHalos/FOF/Masses\"][:]\n",
    "    NoofBoundParticles=handle[\"InputHalos/NumberOfBoundParticles\"][:]\n",
    "    NoofDMParticles=handle[\"ExclusiveSphere/100kpc/NumberOfDarkMatterParticles\"][:]\n",
    "    COM=handle[\"ExclusiveSphere/100kpc/CentreOfMass\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20e663-d5fd-478a-8187-24d1346c6669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'HOST_FOF' :  HOSTFOFID,\n",
    "    'HostHaloIndex':HOSTHALOINDEX, # -1 for central halos\n",
    "    'Catalogue Index': HaloCatalogueIndex,\n",
    "    'Track ID' : Trackid,\n",
    "    'mass':TotalMass,\n",
    "    'FOFMass':FOFMass,\n",
    "    'COM v- x':COMvelocity[:,0],\n",
    "    'COM v- y':COMvelocity[:,1],\n",
    "    'COM v- z':COMvelocity[:,2],\n",
    "    'COM - x':COM[:,0],\n",
    "    'COM - y':COM[:,1],\n",
    "    'COM - z':COM[:,2],\n",
    "    'Bound Particles No':NoofBoundParticles,\n",
    "    'DM Particles No': NoofDMParticles\n",
    "})\n",
    "df['INDEX_HOST_HALOS']=np.asarray(df.index)\n",
    "\n",
    "DF_BOUND_NO_FILTERED= df[df['Bound Particles No']>100]\n",
    "# display(df)\n",
    "# display(DF_BOUND_NO_FILTERED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d873d-50cb-4175-a4f6-1881afc5d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_velocity(df,lower_mass,upper_mass):\n",
    "    #filtering FOF groups based on mass\n",
    "    if upper_mass==-1:\n",
    "        M_bin=df[df['FOFMass']>=lower_mass] \n",
    "    elif lower_mass==-1:\n",
    "        M_bin=df[(df['FOFMass']<upper_mass)]\n",
    "    else:\n",
    "        M_bin=df[(df['FOFMass']>=lower_mass)& (df['FOFMass']<upper_mass)] \n",
    "    HOSTINDICES=np.asarray(M_bin.index) #indices of hosts\n",
    "    display(M_bin)\n",
    "    print(len(HOSTINDICES)) #check number of host halos\n",
    "    filtereddf=df.loc[df['HostHaloIndex'].isin(HOSTINDICES)].sort_values(by='HostHaloIndex') #filters all subhalos within each FOF group in M_bin\n",
    "    print(min(filtereddf['HostHaloIndex']))\n",
    "    print(len(np.unique(np.array(filtereddf['HostHaloIndex']))))\n",
    "    # finding number of subhalos in each FOF group\n",
    "    filtereddf['freq'] = filtereddf.groupby('HostHaloIndex')['HostHaloIndex'].transform('count')\n",
    "    display(filtereddf)\n",
    "    df2=filtereddf.groupby('HostHaloIndex')['freq'].mean()\n",
    "    FINALHOSTINDICES=df2.index\n",
    "    offsets=df2.values.astype(int)\n",
    "\n",
    "    M_bin=M_bin[M_bin['INDEX_HOST_HALOS'].isin(FINALHOSTINDICES)] #making sure that the groups corresponding to the subhalos are used \n",
    "    HOSTHALO_INDICES=np.asarray(M_bin.index)\n",
    "\n",
    "    #center of mass vel of each group\n",
    "    HOSTHALOVEL_X=np.asarray(M_bin['COM v- x'])\n",
    "    HOSTHALOVEL_Y=np.asarray(M_bin['COM v- y'])\n",
    "    HOSTHALOVEL_Z=np.asarray(M_bin['COM v- z'])\n",
    "    #positions\n",
    "    HOSTHALO_X=np.asarray(M_bin['COM - x'])\n",
    "    HOSTHALO_Y=np.asarray(M_bin['COM - y'])\n",
    "    HOSTHALO_Z=np.asarray(M_bin['COM - z'])\n",
    "    #center of mass vel of each subhalo\n",
    "    SUBHALO_X=np.asarray(filtereddf['COM v- x'])\n",
    "    SUBHALO_Y=np.asarray(filtereddf['COM v- y'])\n",
    "    SUBHALO_Z=np.asarray(filtereddf['COM v- z'])\n",
    "    \n",
    "    #subhalo positions\n",
    "    SUBHALO_x=np.asarray(filtereddf['COM - x'])\n",
    "    SUBHALO_y=np.asarray(filtereddf['COM - y'])\n",
    "    SUBHALO_z=np.asarray(filtereddf['COM - z'])\n",
    "\n",
    "    #relative quantities\n",
    "    subhalo_relvel_x=np.zeros(len(SUBHALO_X))\n",
    "    subhalo_relvel_y=np.zeros(len(SUBHALO_X))\n",
    "    subhalo_relvel_z=np.zeros(len(SUBHALO_X))\n",
    "    \n",
    "    subhalo_rel_x=np.zeros(len(SUBHALO_X))\n",
    "    subhalo_rel_y=np.zeros(len(SUBHALO_X))\n",
    "    subhalo_rel_z=np.zeros(len(SUBHALO_X))\n",
    "    print(len(HOSTHALO_INDICES), len(HOSTHALOVEL_X)) #check \n",
    "    \n",
    "    count_lower=0\n",
    "    count_upper=0\n",
    "    for i in range(len(HOSTHALO_INDICES)):\n",
    "        CENTRAL_VELX=HOSTHALOVEL_X[i]\n",
    "        CENTRAL_VELY=HOSTHALOVEL_Y[i]\n",
    "        CENTRAL_VELZ=HOSTHALOVEL_Z[i]\n",
    "        CENTRAL_X=HOSTHALO_X[i]\n",
    "        CENTRAL_Y=HOSTHALO_Y[i]\n",
    "        CENTRAL_Z=HOSTHALO_Z[i]\n",
    "        count_upper+=offsets[i]\n",
    "        # print(count_lower,count_upper) - check if needed\n",
    "        subhalo_relvel_x[count_lower:count_upper]=SUBHALO_X[count_lower:count_upper]-CENTRAL_VELX\n",
    "        subhalo_relvel_y[count_lower:count_upper]=SUBHALO_Y[count_lower:count_upper]-CENTRAL_VELY\n",
    "        subhalo_relvel_z[count_lower:count_upper]=SUBHALO_Z[count_lower:count_upper]-CENTRAL_VELZ\n",
    "        subhalo_rel_x[count_lower:count_upper]=CENTRAL_X- SUBHALO_x[count_lower:count_upper]\n",
    "        subhalo_rel_y[count_lower:count_upper]=CENTRAL_Y-SUBHALO_y[count_lower:count_upper]\n",
    "        subhalo_rel_z[count_lower:count_upper]=CENTRAL_Z-SUBHALO_z[count_lower:count_upper]\n",
    "        count_lower+=offsets[i]\n",
    "    \n",
    "    # print(count_lower,count_upper) - check if needed\n",
    "    \n",
    "    final_velx=subhalo_relvel_x\n",
    "    final_vely=subhalo_relvel_y\n",
    "    final_velz=subhalo_relvel_z\n",
    "    final_relx=subhalo_rel_x\n",
    "    final_rely=subhalo_rel_y\n",
    "    final_relz=subhalo_rel_z\n",
    "    total_vel_xyz=np.concatenate((final_velx,final_vely,final_velz), axis=None) \n",
    "\n",
    "    return final_velx,final_vely,final_velz,total_vel_xyz,final_relx,final_rely,final_relz\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae470a8-0d89-4cf3-a013-b7ee90f10fcb",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76848a4f-04c6-4b55-898c-06e1251dd66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x,mu,sigma):\n",
    "    return  np.exp(-0.5 * ((x -mu) / sigma) ** 2)\n",
    "\n",
    "def gaussian_integral(mu,sigma,x_i,x_f):\n",
    "    integral,_=quad(lambda x: gaussian(x,mu,sigma), x_i, x_f)\n",
    "    return integral\n",
    "    \n",
    "\n",
    "def gaussian_neg_log_likelihood_binned(params, bin_edges, bin_heights, bin_width):\n",
    "    mu,sigma=params\n",
    "    hist_area=np.sum(bin_heights) \n",
    "    fit_integral =(sigma * np.sqrt(2 * np.pi))\n",
    "    A=hist_area/ fit_integral\n",
    "    neg_log_L=0\n",
    "    for i in range(1,len(bin_edges)):\n",
    "        f_b= A* gaussian_integral(mu,sigma,bin_edges[i-1],bin_edges[i])\n",
    "        #penalize negative values and zero\n",
    "        if f_b<=0:\n",
    "            return 10**11\n",
    "        n_b=bin_heights[i-1] * bin_width\n",
    "        neg_log_L+= f_b - (n_b * np.log(f_b))\n",
    "    return neg_log_L\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fb12f-8316-4865-86cc-ec04ea1caea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace(x, mu, b): \n",
    "    return (1 / (2 * b)) * np.exp(-np.abs(x - mu) / b)\n",
    "\n",
    "def laplace_integral(mu,b,x_i,x_f):\n",
    "    integral,_=quad(lambda x: laplace(x, mu, b), x_i, x_f)\n",
    "    return integral\n",
    "\n",
    "def laplace_neg_log_likelihood_binned(params, bin_edges, bin_heights, bin_width):\n",
    "    mu,b=params\n",
    "    hist_area=np.sum(bin_heights) \n",
    "    fit_integral =laplace_integral(mu,b,-np.inf,np.inf)\n",
    "    A=hist_area/ fit_integral\n",
    "    neg_log_L=0\n",
    "    for i in range(1,len(bin_edges)):\n",
    "        f_b= A* laplace_integral(mu,b,bin_edges[i-1],bin_edges[i])\n",
    "        if f_b<0:\n",
    "            return 10**11\n",
    "        n_b=bin_heights[i-1] * bin_width\n",
    "        neg_log_L+= f_b - (n_b * np.log(f_b))\n",
    "    return neg_log_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829620f-78ba-4639-b35d-884385ed9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorentzian(x,mu,gamma):\n",
    "    return 1/ ((1+((x-mu)/gamma)**2))\n",
    "\n",
    "\n",
    "def lorentzian_integral(mu,gamma,x_i,x_f):\n",
    "    integral,_=quad(lambda x: lorentzian(x,mu,gamma), x_i, x_f)\n",
    "    return integral\n",
    "\n",
    "\n",
    "def lorentzian_neg_log_likelihood_binned(params, bin_edges, bin_heights, bin_width):\n",
    "    mu,gamma=params\n",
    "    hist_area=np.sum(bin_heights)\n",
    "    fit_integral=np.pi*gamma\n",
    "    A=hist_area/ fit_integral\n",
    "    neg_log_L=0\n",
    "    for i in range(1,len(bin_edges)):\n",
    "        f_b= A*lorentzian_integral(mu,gamma,bin_edges[i-1],bin_edges[i])\n",
    "        if f_b<0:\n",
    "            return 10**11\n",
    "        n_b=bin_heights[i-1] \n",
    "        neg_log_L+= f_b - (n_b * np.log(f_b))\n",
    "    return neg_log_L\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70339edf",
   "metadata": {},
   "source": [
    "### Sum of two gaussians (best model in the thesis):-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef59919a-74ed-48e2-a7c4-6cb182f8c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_gaussian(x,sigma,sigma1,lambda_):\n",
    "    return ((1-lambda_)* (np.exp(-(x)**2 / (2 * sigma**2)) /sigma ) +lambda_*np.exp(-((x)**2/(2*sigma1**2)))/sigma1)/ (np.sqrt(2*np.pi))\n",
    "\n",
    "def mod_gaussian_integral(sigma,lambda_,lambda2,x_i,x_f):\n",
    "    integral,_=quad(lambda x: mod_gaussian(x,sigma,lambda_,lambda2), x_i, x_f)\n",
    "    return integral\n",
    "    \n",
    "\n",
    "def mod_gaussian_neg_log_likelihood_binned(params, bin_edges, bin_heights, bin_width):\n",
    "    sigma,lambda_,lambda2=params\n",
    "    hist_area=np.sum(bin_heights) \n",
    "    fit_integral =(sigma * np.sqrt(2 * np.pi)) *(3*lambda_ + 1 + 105*lambda2)\n",
    "    A=hist_area/ fit_integral\n",
    "    neg_log_L=0\n",
    "    for i in range(1,len(bin_edges)):\n",
    "        f_b= A* mod_gaussian_integral(sigma,lambda_,lambda2,bin_edges[i-1],bin_edges[i])\n",
    "        #penalize negative values and zero\n",
    "        if f_b<=0:\n",
    "            return 10**11\n",
    "        n_b=bin_heights[i-1] * bin_width\n",
    "        neg_log_L+= f_b - (n_b * np.log(f_b))\n",
    "    return neg_log_L\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ced1fc-35db-4d78-a1f4-b85dfced9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_gaussian_mod(f,integral_f,params,data,bins,distname):\n",
    "    \n",
    "    bin_heights, bin_edges = np.histogram(data, bins=bins, density=False)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    bin_width= bin_edges[1] - bin_edges[0] \n",
    "    bin_widths = np.diff(bin_edges)  # The width of each bin\n",
    "    number_density = bin_heights / bin_widths  # Normalize by bin width\n",
    "    # Plot the histogram\n",
    "    fig=figure(figsize=(7,7))\n",
    "    frame=fig.add_subplot(1,1,1)\n",
    "    frame.set_xlabel('Velocity difference v', fontsize=13)\n",
    "    frame.set_ylabel('Number of galaxies per v', fontsize=13)\n",
    "    frame.bar(bin_centers, number_density, width=bin_width, align='center')\n",
    "    DAT=np.linspace(np.min(data),np.max(data),1000)\n",
    "    hist_area=np.sum(bin_heights)\n",
    "    fit_integral = integral_f(*params,-np.inf,np.inf) \n",
    "    A=hist_area/ fit_integral\n",
    "    frame.plot(DAT,A*f(DAT,*params),'-', label=distname,color='red')\n",
    "    # frame.set_yscale(\"log\")\n",
    "    frame.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59deb8d-6e3e-41ae-9bc3-26bd7120c7cc",
   "metadata": {},
   "source": [
    "## trial stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f43693-c3d0-4462-864a-740e14cd73d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M15_final_velx,M15_final_vely,M15_final_velz,M15_total_Vel,M15_final_x,M15_final_y,M15_final_z=final_velocity(DF_BOUND_NO_FILTERED,10**3.5,10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857bb66-91c6-4cf7-98c7-f091f8becec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scale=pd.DataFrame({\n",
    "    'Relative pos - x':M15_final_x,\n",
    "    'Relative pos - y':M15_final_y,\n",
    "    'Relative pos - z':M15_final_z,\n",
    "    'Relative radial distance':np.sqrt(M15_final_x**2+M15_final_y**2+M15_final_z**2),\n",
    "    'Relative velocity - x':M15_final_velx,\n",
    "    'Relative velocity - y':M15_final_vely,\n",
    "    'Relative velocity - z':M15_final_velz,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c3fcb-0697-421e-ad8d-cff4258df18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df_scale.sort_values(by=['Relative radial distance'])\n",
    "\n",
    "len(df2['Relative velocity - z']) + len(df2['Relative velocity - x']) + len(df2['Relative velocity - y'])\n",
    "print(np.array(df2['Relative radial distance']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af86456-b595-40c2-815c-06f8f50f7c38",
   "metadata": {},
   "source": [
    "## Trial - binned \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c44c057-287c-4504-a421-ba814408b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values = df2['Relative radial distance']  \n",
    "x_values =df2['Relative velocity - x']\n",
    "y_values=df2['Relative velocity - y']\n",
    "z_values= df2['Relative velocity - z']\n",
    "r_bins=[(0,0.16),(0.16,0.225),(0.23,0.29),(0.29,0.325),(0.325,0.3625),(0.3625,0.4),(0.4,0.45),(0.45,0.5),(0.5,0.55),(0.55,0.6),(0.6,0.65),(0.65,0.7),(0.7,0.8),(0.8,1.0),(1.0,1.2),(1.2,5)] # for now\n",
    "unbinned_velocity_values_in_each_rbin=[]\n",
    "def bin_x_values(r_values, x_values, y_values,z_values, r_bins):\n",
    "    binned_data = []\n",
    "    c=0\n",
    "    for r_min, r_max in r_bins:\n",
    "        mask_r = (r_values >= r_min) & (r_values < r_max)\n",
    "        x_vals = x_values[mask_r] \n",
    "        y_vals= y_values[mask_r]\n",
    "        z_vals= z_values[mask_r]\n",
    "        unbinned_velocity_values_in_each_rbin.append(np.concatenate((x_vals,y_vals,z_vals), axis=None))\n",
    "        counts, bins = np.histogram(np.concatenate((x_vals,y_vals,z_vals), axis=None) , bins=80) \n",
    "        x_bins_list = [(float(bins[i]), float(bins[i+1]), int(counts[i])) for i in range(len(counts))]\n",
    "        binned_data.append((float(r_min), float(r_max), x_bins_list))\n",
    "    return binned_data\n",
    "\n",
    "binned_data = bin_x_values(r_values, x_values, y_values,z_values, r_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f3c8e-e07e-4ac3-9d47-4bf05cc11b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_x(x, sigma, sigma1, lambda_):\n",
    "    return (((1-lambda_)* (np.exp(-(x)**2 / (2 * sigma**2)) /sigma )) + ((lambda_*np.exp(-((x)**2/(2*sigma1**2))))/sigma1)) / (np.sqrt(2*np.pi))\n",
    "    \n",
    "# Scale-dependent parameter functions \n",
    "def sigma_r(r, a, b, c,n):\n",
    "    return a * r**n + b * r + c\n",
    "\n",
    "\n",
    "def sigma1_r(r, m, b):\n",
    "    return m * r + b\n",
    "\n",
    "\n",
    "def lambda_r(r, A, B, C):\n",
    "    return A * np.exp(-B * r) + C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e4d19-02a6-43e1-9dda-c9e685f6fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Computing expected count in a single (r, x) bin\n",
    "\n",
    "\n",
    "def expected_count_single_bin(r_min, r_max, x_min, x_max, params):\n",
    "    def integrand(x, r):\n",
    "        sigma = sigma_r(r, *params[:4])  \n",
    "        sigma1 = sigma1_r(r, *params[4:6]) \n",
    "        lam = lambda_r(r, *params[6:9])   \n",
    "        return f_x(x, sigma, sigma1, lam)  \n",
    "    #inner - x integral, outer integral- r\n",
    "    total, _ = dblquad(integrand, r_min, r_max, lambda r: x_min, lambda r: x_max)\n",
    "    return total\n",
    "\n",
    "\n",
    "def neg_log_likelihood(params, binned_data):\n",
    "    logL = 0\n",
    "    for r_min, r_max, x_bins in binned_data:\n",
    "        for (x_min, x_max, N_obs) in x_bins:\n",
    "            P = expected_count_single_bin(r_min, r_max, x_min, x_max, params)\n",
    "            if P <= 0:\n",
    "                return 1e10  # Avoid log(0), should not be happening though\n",
    "            logL += N_obs * np.log(P) - P\n",
    "    return -logL\n",
    "\n",
    "\n",
    "# initial_params = [] - provide initial guesses for each parameter!\n",
    "# Minimization\n",
    "result = minimize(neg_log_likelihood, initial_params, args=(binned_data,),bounds=[..]) #Provide bounds for each parameter!\n",
    "\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b545784e-8565-4db1-90af-2745986db288",
   "metadata": {},
   "source": [
    "# Plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16127a5b-ab7e-49c6-944c-0120b51d4d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = result.x\n",
    "print(\"Optimized parameters:\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c184f31-b88e-48ea-9cf6-5c9b67a216e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_max=5 # Mpc - provide upper limit of radial separation\n",
    "\n",
    "#Marginalize dist with uniform P(r) = 1/r_max\n",
    "def marginalized_P_x(x, params, r_max):\n",
    "    integral_r, _ = quad(lambda r: f_x(x, sigma_r(r, *params[:4]),sigma1_r(r, *params[4:6]), lambda_r(r, *params[6:9])),\n",
    "                         0, r_max)\n",
    "    return integral_r / r_max  \n",
    "\n",
    "\n",
    "x_range = np.linspace(-3000, 3000, 6000)\n",
    "marginalized_dist = np.array([marginalized_P_x(x, params, r_max) for x in x_range])\n",
    "\n",
    "total_count = sum(N_obs for _, _, x_bins in binned_data for (_, _, N_obs) in x_bins)\n",
    "print(\"Total observed count:\", total_count)\n",
    "\n",
    "\n",
    "def normalization_const_r(params, r_max):\n",
    "    #f(y,x) form, with y being the inner integral variable!!\n",
    "    def integrand(r, x, params):\n",
    "        sigma = sigma_r(r, *params[:4])    \n",
    "        sigma1 = sigma1_r(r, *params[4:6]) \n",
    "        lam = lambda_r(r, *params[6:9]) \n",
    "        return f_x(x, sigma, sigma1, lam)  \n",
    "\n",
    "    # Integrate over r first then x\n",
    "    total_area, _ = dblquad(integrand, -np.inf, np.inf, lambda x: 0, lambda x: r_max, args=(params,))\n",
    "    return total_area / (r_max)\n",
    "    \n",
    "A = normalization_const_r(params, r_max)\n",
    "print(\"Normalization constant:\", A)\n",
    "\n",
    "scaled_p = marginalized_dist * total_count / (A )\n",
    "plt.plot(x_range, color=\"red\", linewidth=2)\n",
    "plt.title(\"Velocity distribution\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "bin_heights, bin_edges = np.histogram(np.concatenate((x_values,y_values,z_values),axis=None),bins=80, density=False)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "bin_width= bin_edges[1] - bin_edges[0] \n",
    "number_density = bin_heights / bin_width\n",
    "plt.bar(bin_centers, number_density, width=bin_width, align='center')\n",
    "plt.plot(x_range, scaled_p, color=\"red\", linewidth=2,label=f\"N={total_count:.0f}\")\n",
    "plt.xlabel(\"Velocity difference v\", fontsize=16)\n",
    "plt.ylabel(\"Number of galaxies per v\", fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', length=6, width=2, labelsize=14)\n",
    "# plt.savefig(\"Marginalized_distribution_allr_M13_5_14.pdf\", dpi=500)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce6691-d8b7-4fbb-b59d-6ee8842f4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginalized_P_x_specific_r(x, params, r_min , r_max):\n",
    "    integral_r, _ = quad(lambda r: f_x(x, sigma_r(r, *params[:4]),sigma1_r(r, *params[4:6]), lambda_r(r, *params[6:9])),\n",
    "                         r_min, r_max)\n",
    "    return integral_r / ((r_max - r_min))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalization_const_specific_r(params, r_min, r_max):\n",
    "    #f(y,x) form with y being the inner integral variable!!\n",
    "    def integrand(r, x, params):\n",
    "        sigma = sigma_r(r, *params[:4])    \n",
    "        sigma1 = sigma1_r(r, *params[4:6]) \n",
    "        lam = lambda_r(r, *params[6:9]) \n",
    "        return f_x(x, sigma, sigma1, lam) \n",
    "\n",
    "    # Integrate over r first then x\n",
    "    total_area, _ = dblquad(integrand, -np.inf, np.inf, lambda x: r_min, lambda x: r_max, args=(params,))\n",
    "    return total_area / (r_max - r_min)\n",
    "\n",
    "\n",
    "def marginalized_P_plots(velocity_values,r_bins,opt_params):\n",
    "    \"\"\"\n",
    "    velocity_values = velocity values inside each r bin (unbinned)\n",
    "    r_bins = list with bin edges of each r bin\n",
    "    opt_params = fitted parameters of scale dependent functions\n",
    "    \"\"\"\n",
    "    # fig=figure(figsize=(10,10))\n",
    "    for i in range(len(r_bins)):\n",
    "        #Calculate normalization constant\n",
    "        fig=figure(figsize=(7,5))\n",
    "        frame=fig.add_subplot(1,1,1)\n",
    "        A = normalization_const_specific_r(opt_params, r_bins[i][0],r_bins[i][1])\n",
    "        # print(f\"Normalization constant for bin {r_bins[i][0]} Mpc - {r_bins[i][1]} Mpc = {A} \")\n",
    "        \n",
    "        #Generate bin-marginalized distribution inside each r bin\n",
    "        x_range = np.linspace(np.min(velocity_values[i]), np.max(velocity_values[i]), 7000)\n",
    "\n",
    "        marginalized_dist_specific_r = np.array([marginalized_P_x_specific_r(x, opt_params, r_bins[i][0],r_bins[i][1]) for x in x_range])\n",
    "\n",
    "        \n",
    "        # frame=fig.add_subplot(len(r_bins),1,i+1 )\n",
    "        bin_heights, bin_edges = np.histogram(velocity_values[i], bins=30, density=False)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        bin_width= bin_edges[1] - bin_edges[0] \n",
    "        number_density = bin_heights / bin_width\n",
    "        hist_area=len(velocity_values[i])\n",
    "        scaled_p = marginalized_dist_specific_r * hist_area / A\n",
    "        frame.bar(bin_centers, number_density, width=bin_width, align='center')\n",
    "        frame.plot(x_range, scaled_p, color=\"red\", linewidth=2, label=f\"N={hist_area:.0f}\")\n",
    "        frame.set_xlabel('Velocity difference v', fontsize=13)\n",
    "        frame.set_ylabel('Number of galaxies per v', fontsize=13)\n",
    "        frame.tick_params(axis='both', which='major', length=6, width=2, labelsize=14)\n",
    "        frame.legend(fontsize=12.5, loc=\"upper right\")\n",
    "        fig.tight_layout()\n",
    "        #TO SAVE IMAGE\n",
    "        # fig.savefig(f\"Marginalized_distribution_allr_M13_5_14__bin_{r_bins[i][0]:.1f}_{r_bins[i][1]:.1f}.pdf\", dpi=500)\n",
    "        show()\n",
    "\n",
    "        \n",
    "marginalized_P_plots(unbinned_velocity_values_in_each_rbin,r_bins,params,10**14,10**14.5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8bff9",
   "metadata": {},
   "source": [
    "Comments from Sowmya:\n",
    "\n",
    "I could have used MCMC fitting procedures instead of the one I used here, so I would suggest adding that instead of what I did. Additionally, I should have explored more combinations of scale-dependent functions for the parameters. Good luck, and I would really appreciate it if you could keep me updated on the progress made in finding/improving the model at the end of your project!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f2f75",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
