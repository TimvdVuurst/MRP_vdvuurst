{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba744b50-7c32-4773-84f7-c656f27ba88a",
   "metadata": {},
   "source": [
    "# Velocity Distributions \n",
    "currently trying out sum of gaussians with 2 different sigmas - $10^{13.5} - 10^{14} M_\\odot$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bcdf28-8e2e-462e-bd82-11af6e69eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import figure,show\n",
    "from swiftsimio import load\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import root, minimize\n",
    "from lmfit.models import LorentzianModel, VoigtModel, GaussianModel,LognormalModel\n",
    "from scipy.integrate import quad\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85a766-d8b6-4bb2-8d7b-61127a493c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_hydro= \"/net/hypernova/data2/FLAMINGO/L1000N1800/HYDRO_FIDUCIAL/SOAP-HBT/halo_properties_0077.hdf5\"\n",
    "with h5py.File(path_hydro, \"r\") as handle:\n",
    "    TotalMass= handle[\"ExclusiveSphere/100kpc/TotalMass\"][:]\n",
    "    COMvelocity=handle[\"ExclusiveSphere/100kpc/CentreOfMassVelocity\"][:]\n",
    "    Trackid=handle[\"InputHalos/HBTplus/TrackId\"][:]\n",
    "    HOSTFOFID=handle[\"InputHalos/HBTplus/HostFOFId\"][:]\n",
    "    HaloCatalogueIndex=handle[\"InputHalos/HaloCatalogueIndex\"][:]\n",
    "    HOSTHALOINDEX=handle[\"SOAP/HostHaloIndex\"][:]\n",
    "    FOFMass=handle[\"InputHalos/FOF/Masses\"][:]\n",
    "    NoofBoundParticles=handle[\"InputHalos/NumberOfBoundParticles\"][:]\n",
    "    NoofDMParticles=handle[\"ExclusiveSphere/100kpc/NumberOfDarkMatterParticles\"][:]\n",
    "    COM=handle[\"ExclusiveSphere/100kpc/CentreOfMass\"][:]\n",
    "    MaxCircularVel=handle[\"BoundSubhalo/MaximumCircularVelocity\"][:]\n",
    "    LastMaxCircularVel=handle[\"InputHalos/HBTplus/LastMaxVmaxPhysical\"][:]\n",
    "    LastMaxMass=handle[\"InputHalos/HBTplus/LastMaxMass\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20e663-d5fd-478a-8187-24d1346c6669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'HOST_FOF' :  HOSTFOFID,\n",
    "    'HostHaloIndex':HOSTHALOINDEX, # -1 for central halos\n",
    "    'Catalogue Index': HaloCatalogueIndex,\n",
    "    'Track ID' : Trackid,\n",
    "    'mass':TotalMass,\n",
    "    'FOFMass':FOFMass,\n",
    "    'COM v- x':COMvelocity[:,0],\n",
    "    'COM v- y':COMvelocity[:,1],\n",
    "    'COM v- z':COMvelocity[:,2],\n",
    "    'COM - x':COM[:,0],\n",
    "    'COM - y':COM[:,1],\n",
    "    'COM - z':COM[:,2],\n",
    "    'Bound Particles No':NoofBoundParticles,\n",
    "    'DM Particles No': NoofDMParticles,\n",
    "    'Max Circular Vel':MaxCircularVel,\n",
    "    'Last Max Mass':LastMaxMass,\n",
    "    'Last Max Circular Vel':LastMaxCircularVel\n",
    "})\n",
    "df['INDEX_HOST_HALOS']=np.asarray(df.index)\n",
    "\n",
    "DF_BOUND_NO_FILTERED= df[df['Bound Particles No']>100]\n",
    "display(df)\n",
    "display(DF_BOUND_NO_FILTERED.sort_values(by='Last Max Mass'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b93506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trial new version. if it does not work, go with the old version!\n",
    "def final_velocity(df, lower_mass=-1, upper_mass=-1):\n",
    "    if upper_mass == -1:\n",
    "        M_bin = df[df['FOFMass'] >= lower_mass]\n",
    "    elif lower_mass == -1:\n",
    "        M_bin = df[df['FOFMass'] < upper_mass]\n",
    "    else:\n",
    "        M_bin = df[(df['FOFMass'] >= lower_mass) & (df['FOFMass'] < upper_mass)]\n",
    "\n",
    "    HOSTINDICES = M_bin.index.to_numpy()\n",
    "    print(f\"Number of halos: {len(HOSTINDICES)}\")\n",
    "\n",
    "    # Filter subhalos of halos\n",
    "    filtereddf = df[df['HostHaloIndex'].isin(HOSTINDICES)].sort_values(by='HostHaloIndex')\n",
    "    filtereddf['freq'] = filtereddf.groupby('HostHaloIndex')['HostHaloIndex'].transform('count')\n",
    "\n",
    "    # Number of subhalos per halo\n",
    "    df2 = filtereddf.groupby('HostHaloIndex')['freq'].mean()\n",
    "    FINALHOSTINDICES = df2.index.to_numpy()\n",
    "    offsets = df2.values.astype(int)\n",
    "\n",
    "    # Keep only halos corresponding to subhalos\n",
    "    M_bin = M_bin[M_bin['INDEX_HOST_HALOS'].isin(FINALHOSTINDICES)]\n",
    "\n",
    "    host_vel_stack = M_bin[['COM v- x', 'COM v- y', 'COM v- z']].to_numpy()\n",
    "    host_pos_stack = M_bin[['COM - x', 'COM - y', 'COM - z']].to_numpy()\n",
    "    subhalo_vel_stack = filtereddf[['COM v- x', 'COM v- y', 'COM v- z']].to_numpy()\n",
    "    subhalo_pos_stack = filtereddf[['COM - x', 'COM - y', 'COM - z']].to_numpy()\n",
    "\n",
    "    host_vel_repeat = np.repeat(host_vel_stack, offsets, axis=0)\n",
    "    host_pos_repeat = np.repeat(host_pos_stack, offsets, axis=0)\n",
    "\n",
    "    # Compute relative velocities and positions\n",
    "    subhalo_relvel = subhalo_vel_stack - host_vel_repeat\n",
    "    subhalo_rel = host_pos_repeat - subhalo_pos_stack\n",
    "\n",
    "    final_velx, final_vely, final_velz = subhalo_relvel.T\n",
    "    final_relx, final_rely, final_relz = subhalo_rel.T\n",
    "    total_vel_xyz = subhalo_relvel.ravel()\n",
    "\n",
    "    return final_velx, final_vely, final_velz, total_vel_xyz, final_relx, final_rely, final_relz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d873d-50cb-4175-a4f6-1881afc5d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old version:\n",
    "# \n",
    "# def final_velocity(df,lower_mass,upper_mass):\n",
    "#     #filtering FOF groups based on mass\n",
    "#     if upper_mass==-1:\n",
    "#         M_bin=df[df['FOFMass']>=lower_mass] \n",
    "#     elif lower_mass==-1:\n",
    "#         M_bin=df[(df['FOFMass']<upper_mass)]\n",
    "#     else:\n",
    "#         M_bin=df[(df['FOFMass']>=lower_mass)& (df['FOFMass']<upper_mass)] \n",
    "#     HOSTINDICES=np.asarray(M_bin.index) #indices of hosts\n",
    "#     display(M_bin)\n",
    "#     print((HOSTINDICES)) #check number of host halos\n",
    "#     filtereddf=df.loc[df['HostHaloIndex'].isin(HOSTINDICES)].sort_values(by='HostHaloIndex') #filters all subhalos within each FOF group in M_bin\n",
    "#     print(min(filtereddf['HostHaloIndex']))\n",
    "#     print(len(np.unique(np.array(filtereddf['HostHaloIndex']))))\n",
    "#     # finding number of subhalos in each FOF group\n",
    "#     filtereddf['freq'] = filtereddf.groupby('HostHaloIndex')['HostHaloIndex'].transform('count')\n",
    "#     display(filtereddf)\n",
    "#     df2=filtereddf.groupby('HostHaloIndex')['freq'].mean()\n",
    "#     FINALHOSTINDICES=df2.index\n",
    "#     offsets=df2.values.astype(int)\n",
    "\n",
    "#     M_bin=M_bin[M_bin['INDEX_HOST_HALOS'].isin(FINALHOSTINDICES)] #making sure that the groups corresponding to the subhalos are used \n",
    "#     HOSTHALO_INDICES=np.asarray(M_bin.index)\n",
    "\n",
    "#     #center of mass vel of each group\n",
    "#     HOSTHALOVEL_X=np.asarray(M_bin['COM v- x'])\n",
    "#     HOSTHALOVEL_Y=np.asarray(M_bin['COM v- y'])\n",
    "#     HOSTHALOVEL_Z=np.asarray(M_bin['COM v- z'])\n",
    "#     #positions\n",
    "#     HOSTHALO_X=np.asarray(M_bin['COM - x'])\n",
    "#     HOSTHALO_Y=np.asarray(M_bin['COM - y'])\n",
    "#     HOSTHALO_Z=np.asarray(M_bin['COM - z'])\n",
    "#     #center of mass vel of each subhalo\n",
    "#     SUBHALO_X=np.asarray(filtereddf['COM v- x'])\n",
    "#     SUBHALO_Y=np.asarray(filtereddf['COM v- y'])\n",
    "#     SUBHALO_Z=np.asarray(filtereddf['COM v- z'])\n",
    "    \n",
    "#     #subhalo positions\n",
    "#     SUBHALO_x=np.asarray(filtereddf['COM - x'])\n",
    "#     SUBHALO_y=np.asarray(filtereddf['COM - y'])\n",
    "#     SUBHALO_z=np.asarray(filtereddf['COM - z'])\n",
    "\n",
    "#     #relative quantities\n",
    "#     subhalo_relvel_x=np.zeros(len(SUBHALO_X))\n",
    "#     subhalo_relvel_y=np.zeros(len(SUBHALO_X))\n",
    "#     subhalo_relvel_z=np.zeros(len(SUBHALO_X))\n",
    "    \n",
    "#     subhalo_rel_x=np.zeros(len(SUBHALO_X))\n",
    "#     subhalo_rel_y=np.zeros(len(SUBHALO_X))\n",
    "#     subhalo_rel_z=np.zeros(len(SUBHALO_X))\n",
    "#     print(len(HOSTHALO_INDICES), len(HOSTHALOVEL_X)) #check \n",
    "    \n",
    "#     count_lower=0\n",
    "#     count_upper=0\n",
    "#     for i in range(len(HOSTHALO_INDICES)):\n",
    "#         CENTRAL_VELX=HOSTHALOVEL_X[i]\n",
    "#         CENTRAL_VELY=HOSTHALOVEL_Y[i]\n",
    "#         CENTRAL_VELZ=HOSTHALOVEL_Z[i]\n",
    "#         CENTRAL_X=HOSTHALO_X[i]\n",
    "#         CENTRAL_Y=HOSTHALO_Y[i]\n",
    "#         CENTRAL_Z=HOSTHALO_Z[i]\n",
    "#         count_upper+=offsets[i]\n",
    "#         # print(count_lower,count_upper)\n",
    "#         subhalo_relvel_x[count_lower:count_upper]=SUBHALO_X[count_lower:count_upper]-CENTRAL_VELX\n",
    "#         subhalo_relvel_y[count_lower:count_upper]=SUBHALO_Y[count_lower:count_upper]-CENTRAL_VELY\n",
    "#         subhalo_relvel_z[count_lower:count_upper]=SUBHALO_Z[count_lower:count_upper]-CENTRAL_VELZ\n",
    "#         subhalo_rel_x[count_lower:count_upper]=CENTRAL_X- SUBHALO_x[count_lower:count_upper]\n",
    "#         subhalo_rel_y[count_lower:count_upper]=CENTRAL_Y-SUBHALO_y[count_lower:count_upper]\n",
    "#         subhalo_rel_z[count_lower:count_upper]=CENTRAL_Z-SUBHALO_z[count_lower:count_upper]\n",
    "#         count_lower+=offsets[i]\n",
    "    \n",
    "#     print(count_lower,count_upper)\n",
    "    \n",
    "#     final_velx=subhalo_relvel_x\n",
    "#     final_vely=subhalo_relvel_y\n",
    "#     final_velz=subhalo_relvel_z\n",
    "#     final_relx=subhalo_rel_x\n",
    "#     final_rely=subhalo_rel_y\n",
    "#     final_relz=subhalo_rel_z\n",
    "#     total_vel_xyz=np.concatenate((final_velx,final_vely,final_velz), axis=None) \n",
    "\n",
    "#     return final_velx,final_vely,final_velz,total_vel_xyz,final_relx,final_rely,final_relz\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae470a8-0d89-4cf3-a013-b7ee90f10fcb",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138fc4b5-9321-4ed8-8a11-a77da1f30565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_gaussian(x,sigma,sigma1,lambda_):\n",
    "    return ((1-lambda_)* (np.exp(-(x)**2 / (2 * sigma**2)) /sigma ) +lambda_*np.exp(-((x)**2/(2*sigma1**2)))/sigma1)/ (np.sqrt(2*np.pi))\n",
    "\n",
    "\n",
    "\n",
    "def mod_gaussian_integral(sigma,sigma1,lambda_,x_i,x_f):\n",
    "    integral,_=quad(lambda x: mod_gaussian(x,sigma,sigma1,lambda_), x_i, x_f)\n",
    "    return integral\n",
    "    \n",
    "\n",
    "def mod_gaussian_neg_log_likelihood_binned(params, bin_edges, bin_heights, bin_width):\n",
    "    sigma,sigma1,lambda_=params\n",
    "    hist_area=np.sum(bin_heights) \n",
    "    fit_integral =1\n",
    "    A=hist_area/ fit_integral\n",
    "    neg_log_L=0\n",
    "    for i in range(1,len(bin_edges)):\n",
    "        f_b= A* mod_gaussian_integral(sigma,sigma1,lambda_,bin_edges[i-1],bin_edges[i])\n",
    "        if f_b<=0:\n",
    "            return 10**11\n",
    "        n_b=bin_heights[i-1] * bin_width\n",
    "        neg_log_L+= f_b - (n_b * np.log(f_b))\n",
    "    return neg_log_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b95b2-f20f-46bc-bfd3-5ded4a774fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_distribution_gaussian_mod(f,params,data,bins,distname,binno):\n",
    "    \n",
    "    bin_heights, bin_edges = np.histogram(data, bins=bins, density=False)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    bin_width= bin_edges[1] - bin_edges[0] \n",
    "    bin_widths = np.diff(bin_edges)  # The width of each bin\n",
    "    number_density = bin_heights / bin_widths  # Normalize by bin width\n",
    "    # Plot the histogram\n",
    "    fig=figure(figsize=(7,7))\n",
    "    frame=fig.add_subplot(1,1,1)\n",
    "    frame.set_xlabel('Velocity difference v', fontsize=16)\n",
    "    frame.set_ylabel('Number of galaxies per v', fontsize=16)\n",
    "    frame.bar(bin_centers, number_density, width=bin_width, align='center')\n",
    "    DAT=np.linspace(np.min(data),np.max(data),1000)\n",
    "    sigma,sigma1,lambda_=params \n",
    "    hist_area=np.sum(bin_heights)\n",
    "    print(hist_area)\n",
    "    fit_integral = 1 #since we're using a normalized function\n",
    "    A=hist_area/ fit_integral\n",
    "    print(A)\n",
    "    frame.plot(DAT,A*f(DAT,sigma,sigma1,lambda_),'-', label=f\"{distname},\\nN={hist_area:.0f}\",color='red')\n",
    "    # frame.set_yscale(\"log\")\n",
    "    frame.legend(fontsize=12.5, loc=\"upper right\" \n",
    "    frame.tick_params(axis='both', which='major',length=6, width=2,labelsize=14)\n",
    "    # fig.savefig(f'M13_5_14_bin_{binno}.pdf', dpi=700)\n",
    "    show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f43693-c3d0-4462-864a-740e14cd73d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M15_final_velx,M15_final_vely,M15_final_velz,M15_total_Vel,M15_final_x,M15_final_y,M15_final_z=final_velocity(DF_BOUND_NO_FILTERED,10**3.5,10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857bb66-91c6-4cf7-98c7-f091f8becec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scale=pd.DataFrame({\n",
    "    'Relative pos - x':M15_final_x,\n",
    "    'Relative pos - y':M15_final_y,\n",
    "    'Relative pos - z':M15_final_z,\n",
    "    'Relative radial distance':np.sqrt(M15_final_x**2+M15_final_y**2+M15_final_z**2),\n",
    "    'Relative velocity - x':M15_final_velx,\n",
    "    'Relative velocity - y':M15_final_vely,\n",
    "    'Relative velocity - z':M15_final_velz,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322496a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df01=df2[df2['Relative radial distance']<0.1]\n",
    "vel_01=np.concatenate((df01['Relative velocity - x'],df01['Relative velocity - y'],df01['Relative velocity - z']), axis=None) \n",
    "\n",
    "\n",
    "df05=df2[(df2['Relative radial distance'] > 0.1) & (df2['Relative radial distance'] < 0.2)]\n",
    "vel_07=np.concatenate((df05['Relative velocity - x'],df05['Relative velocity - y'],df05['Relative velocity - z']), axis=None) \n",
    "\n",
    "\n",
    "\n",
    "df10=df2[(df2['Relative radial distance']>0.2) & (df2['Relative radial distance'] < 0.23)]\n",
    "vel_10=np.concatenate((df10['Relative velocity - x'],df10['Relative velocity - y'],df10['Relative velocity - z']), axis=None)\n",
    "\n",
    "df11=df2[(df2['Relative radial distance']>0.23) & (df2['Relative radial distance'] < 0.29)]\n",
    "vel_11=np.concatenate((df11['Relative velocity - x'],df11['Relative velocity - y'],df11['Relative velocity - z']), axis=None)\n",
    "\n",
    "df12=df2[(df2['Relative radial distance']>0.29) & (df2['Relative radial distance'] < 0.32)]\n",
    "vel_12=np.concatenate((df12['Relative velocity - x'],df12['Relative velocity - y'],df12['Relative velocity - z']), axis=None)\n",
    "\n",
    "df13=df2[(df2['Relative radial distance']>0.32) & (df2['Relative radial distance'] < 0.36)]\n",
    "vel_13=np.concatenate((df13['Relative velocity - x'],df13['Relative velocity - y'],df13['Relative velocity - z']), axis=None)\n",
    "\n",
    "df14=df2[(df2['Relative radial distance']>0.36) & (df2['Relative radial distance'] < 0.4)]\n",
    "vel_14=np.concatenate((df14['Relative velocity - x'],df14['Relative velocity - y'],df14['Relative velocity - z']), axis=None)\n",
    "\n",
    "df15=df2[(df2['Relative radial distance'] > 0.40) & (df2['Relative radial distance'] < 0.50)]\n",
    "vel_15=np.concatenate((df15['Relative velocity - x'],df15['Relative velocity - y'],df15['Relative velocity - z']), axis=None) \n",
    "\n",
    "df16=df2[(df2['Relative radial distance'] > 0.50) & (df2['Relative radial distance'] < 0.60)]\n",
    "vel_16=np.concatenate((df16['Relative velocity - x'],df16['Relative velocity - y'],df16['Relative velocity - z']), axis=None) \n",
    "\n",
    "df17=df2[(df2['Relative radial distance'] > 0.60) & (df2['Relative radial distance'] < 0.80)]\n",
    "vel_17=np.concatenate((df10['Relative velocity - x'],df17['Relative velocity - y'],df17['Relative velocity - z']), axis=None)\n",
    "\n",
    "df20=df2[(df2['Relative radial distance'] > 0.80) & (df2['Relative radial distance'] < 1.0)]\n",
    "vel_20=np.concatenate((df20['Relative velocity - x'],df20['Relative velocity - y'],df20['Relative velocity - z']), axis=None) \n",
    "\n",
    "df23=df2[(df2['Relative radial distance'] > 1.0) & (df2['Relative radial distance'] < 1.2)]\n",
    "vel_23=np.concatenate((df23['Relative velocity - x'],df23['Relative velocity - y'],df23['Relative velocity - z']), axis=None) \n",
    "\n",
    "df25=df2[(df2['Relative radial distance'] > 1.2) & (df2['Relative radial distance'] < 1.4)]\n",
    "vel_25=np.concatenate((df25['Relative velocity - x'],df25['Relative velocity - y'],df25['Relative velocity - z']), axis=None) \n",
    "\n",
    "df27=df2[(df2['Relative radial distance'] > 1.4) & (df2['Relative radial distance'] < 1.6)]\n",
    "vel_27=np.concatenate((df27['Relative velocity - x'],df27['Relative velocity - y'],df27['Relative velocity - z']), axis=None) \n",
    "  \n",
    "df30=df2[(df2['Relative radial distance'] > 1.6) & (df2['Relative radial distance'] < 1.8)]\n",
    "vel_30=np.concatenate((df30['Relative velocity - x'],df30['Relative velocity - y'],df30['Relative velocity - z']), axis=None) \n",
    "\n",
    "df40=df2[(df2['Relative radial distance'] > 1.8) & (df2['Relative radial distance'] < 2.0)]\n",
    "vel_40=np.concatenate((df40['Relative velocity - x'],df40['Relative velocity - y'],df40['Relative velocity - z']), axis=None) \n",
    "\n",
    "df45=df2[(df2['Relative radial distance'] > 2.0) & (df2['Relative radial distance'] < 2.2)]\n",
    "vel_45=np.concatenate((df45['Relative velocity - x'],df45['Relative velocity - y'],df45['Relative velocity - z']), axis=None) \n",
    "\n",
    "df47=df2[(df2['Relative radial distance'] > 2.2) & (df2['Relative radial distance'] < 2.7)]\n",
    "vel_47=np.concatenate((df47['Relative velocity - x'],df47['Relative velocity - y'],df47['Relative velocity - z']), axis=None) \n",
    "\n",
    "\n",
    "df50=df2[(df2['Relative radial distance'] > 2.7) & (df2['Relative radial distance'] < 5)]\n",
    "vel_50=np.concatenate((df50['Relative velocity - x'],df50['Relative velocity - y'],df50['Relative velocity - z']), axis=None) \n",
    "\n",
    "# print(len(vel_01),len(vel_07),len(vel_10),len(vel_11),len(vel_12),len(vel_13),len(vel_14),len(vel_15),len(vel_16),len(vel_17),len(vel_20),len(vel_25),len(vel_27),len(vel_30),len(vel_40),len(vel_45),len(vel_47),len(vel_50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fit_modified_gaussian(data,bins,initial_guess,bounds, neg_log_likelihood_func,plot_func=None,dist_func=None,distname='Modified Gaussian',binno=1):\n",
    "    \"\"\"\n",
    "    Fits a modified Gaussian distribution using minimize (scipy.optimize) to binned data and plots the result.\n",
    "\n",
    "    Input:\n",
    "    data : Input peculiar velocity difference data to fit.\n",
    "    bins : [int] Number of histogram bins.\n",
    "    initial_guess : Initial guess for the parameters.\n",
    "    bounds :  Bounds for the parameters .\n",
    "    neg_log_likelihood_func : Negative log_likelihood function to minimize (negative log-likelihood). Must use (bin_edges, bin_heights, bin_width, result.x).\n",
    "    plot_func : Function to plot the results. Must use (dist_func, params, data, bins, distname, binno).\n",
    "    dist_func :  optional. Distribution function to be used while plotting.\n",
    "    distname : [str] optional.  Name of the distribution to display in the plot.\n",
    "    binno : [int or str] optional.  Identifier of the bin for labeling.\n",
    "\n",
    "    Returns:\n",
    "    result : The optimization result object from minimize.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute histogram\n",
    "    bin_heights, bin_edges = np.histogram(data, bins=bins, density=False)\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "\n",
    "    # Optimize\n",
    "    result = minimize(\n",
    "        neg_log_likelihood_func,\n",
    "        initial_guess,\n",
    "        args=(bin_edges, bin_heights, bin_width),\n",
    "        bounds=bounds\n",
    "    )\n",
    "\n",
    "    print(\"Optimized parameters:\", result.x)\n",
    "    print(result)\n",
    "\n",
    "    # Plot if function provided\n",
    "    if plot_func and dist_func:\n",
    "        plot_func(dist_func, result.x, data, bins=bins, distname=distname, binno=binno)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c6c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit_and_plot_multiple_gaussians(datasets, bins, initial_guesses, bounds=[(1e-10, None), (1e-10, None), (0.1, None)],neg_log_likelihood_func=None,plot_func=None,dist_func=None,distname='Modified Gaussian',bin_labels=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits a modified Gaussian distribution to multiple datasets, plots each result,\n",
    "    and returns a dictionary of optimization results.\n",
    "\n",
    "    Input:\n",
    "    datasets : List of peculiar velocity difference datasets to fit.\n",
    "    bins : [int] Number of histogram bins.\n",
    "    initial_guesses : [list or list of lists]\n",
    "        If single list-> will be used for all datasets.\n",
    "        If list of lists-> each sublist must be an initial guess for corresponding dataset.\n",
    "    bounds : [list or list of lists]\n",
    "        If single list-> will be used for all datasets.\n",
    "        If list of lists-> each sublist must be an initial guess for corresponding dataset.\n",
    "    neg_log_likelihood_func :  Negative log-likelihood function to minimize .\n",
    "    plot_func :  Function to plot the model with best-fit parameters and the histogram of the underlying data for each dataset.\n",
    "    dist_func :distribution function to be used within plot_func.\n",
    "    distname : [str, optional] Name of the distribution to display in plot(s).\n",
    "    bin_labels : [list of str or int, optional] List of labels for each dataset. If None, indices 01,02... will be assigned.\n",
    "\n",
    "    Returns:\n",
    "    results : [dictionary] {label: result}, where each key is the label of the corresponding dataset, and the value assigned to the key is the result of minimization\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Generate labels for saving images of plots\n",
    "    if bin_labels is None:\n",
    "        bin_labels = [f\"{i+1:02}\" for i in range(len(datasets))]\n",
    "\n",
    "    if isinstance(initial_guesses[0], (int, float)):\n",
    "        initial_guesses = [initial_guesses] * len(datasets)\n",
    "    if isinstance(bounds[0][0], (int, float, type(None))):\n",
    "        bounds = [bounds] * len(datasets)\n",
    "\n",
    "    for data, label, initial_guess, bound in zip(datasets, bin_labels, initial_guesses, bounds):\n",
    "        bin_heights, bin_edges = np.histogram(data, bins=bins, density=False)\n",
    "        bin_width = bin_edges[1] - bin_edges[0]\n",
    "\n",
    "        result = minimize(neg_log_likelihood_func, initial_guess,args=(bin_edges, bin_heights, bin_width),bounds=bound)\n",
    "\n",
    "        # print(\"Optimized parameters:\", result.x)\n",
    "        # print(result)\n",
    "\n",
    "        if plot_func and dist_func:\n",
    "            plot_func(dist_func, result.x, data, bins=bins, distname=distname, binno=label)\n",
    "\n",
    "        results[label] = result\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4cf3d8",
   "metadata": {},
   "source": [
    "examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002335a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gaussian_mod_47 = fit_and_plot_modified_gaussian(\n",
    "    data=vel_47,\n",
    "    bins=70,\n",
    "    initial_guess=[700, 100, -0.02],\n",
    "    bounds=[(0.01, None), (0.0001, None), (-0.09, 1)],\n",
    "    neg_log_likelihood_func=mod_gaussian_neg_log_likelihood_binned,\n",
    "    plot_func=plot_distribution_gaussian_mod,\n",
    "    dist_func=mod_gaussian,\n",
    "    distname='Modified Gaussian',\n",
    "    binno='47'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e7c07-20d7-4d96-a94b-adf08a935041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without using the functions - repeat for all of the velocity arrays above!\n",
    "bin_heights_01, bin_edges_01 = np.histogram(vel_01, bins=70, density=False)\n",
    "bin_centers_01 = (bin_edges_01[:-1] + bin_edges_01[1:]) / 2\n",
    "bin_width_01= bin_edges_01[1] - bin_edges_01[0] \n",
    "\n",
    "initial_guess_mod_gauss=[300,100,0.5]\n",
    "result_gaussian_mod_01= minimize(mod_gaussian_neg_log_likelihood_binned, initial_guess_mod_gauss, args=(bin_edges_01, bin_heights_01, bin_width_01),bounds=[(0.0000000001,None),(0.0000000001,None),(0.1,None)])\n",
    "print(\"Optimized parameters:\", result_gaussian_mod_01.x)\n",
    "print(result_gaussian_mod_01)\n",
    "plot_distribution_gaussian_mod(mod_gaussian,result_gaussian_mod_01.x,vel_01,bins=70\n",
    "                               ,distname='Modified Gaussian',binno='01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0a0a1-a145-47ba-9151-b1bd9b8c3ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_heights_07, bin_edges_07 = np.histogram(vel_07, bins=100, density=False)\n",
    "bin_centers_07 = (bin_edges_07[:-1] + bin_edges_07[1:]) / 2\n",
    "bin_width_07= bin_edges_07[1] - bin_edges_07[0] \n",
    "\n",
    "initial_guess_mod_gauss=[300,100,0.5]\n",
    "result_gaussian_mod_07= minimize(mod_gaussian_neg_log_likelihood_binned, initial_guess_mod_gauss, args=(bin_edges_07, bin_heights_07, bin_width_07),bounds=[(0.0000000001,None),(0.0000000001,None),(-100,None)])\n",
    "print(\"Optimized parameters:\", result_gaussian_mod_07.x)\n",
    "print(result_gaussian_mod_07)\n",
    "plot_distribution_gaussian_mod(mod_gaussian,result_gaussian_mod_07.x,vel_07,bins=40\n",
    "                               ,distname='Modified Gaussian',binno='07')\n",
    "\n",
    "\n",
    "# [ 3.959e+02  8.404e+01  6.849e-02]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c37a9",
   "metadata": {},
   "source": [
    "Plotting kurtosis of model with best-fit parameters vs scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b781037-efac-4d27-8cbb-98fb38ee24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results=np.asarray([result_gaussian_mod_01.x,result_gaussian_mod_07.x,result_gaussian_mod_10.x, result_gaussian_mod_11.x,result_gaussian_mod_12.x,result_gaussian_mod_13.x,\n",
    "                    result_gaussian_mod_14.x,result_gaussian_mod_15.x,result_gaussian_mod_16.x,result_gaussian_mod_17.x,result_gaussian_mod_20.x,result_gaussian_mod_23.x,\n",
    "                    result_gaussian_mod_25.x,\n",
    "                    result_gaussian_mod_27.x,result_gaussian_mod_30.x,result_gaussian_mod_40.x,result_gaussian_mod_45.x,result_gaussian_mod_47.x,result_gaussian_mod_50.x,])\n",
    "\n",
    "print(results[:,0])\n",
    "results\n",
    "r_bins=[(0,0.1),(0.1,0.2),(0.2,0.23),(0.23,0.29),(0.29,0.32),(0.32,0.36),(0.36,0.4),(0.4,0.5),(0.5,0.6),(0.6,0.8),(0.8,0.1),(1,1.2),(1.2,1.4), (1.4,1.6),(1.6,1.8),(1.8,2.0),(2,2.2),(2.2,2.7),(2.7,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641525b8-febe-4f61-bb2f-8a5a85deb179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def kurtosis_vs_radius(r_bins, lambdas, sigma, sigma1, plot=True):\n",
    "    \"\"\"\n",
    "    Computes and plots excess kurtosis of a weighted sum of two Gaussians vs radius.\n",
    "    \n",
    "    Input:\n",
    "        r_bins : List of (r_min, r_max) of all radial bins\n",
    "        lambdas : Lambda values for all radial bins.\n",
    "        sigma : Sigma values for all radial bins\n",
    "        sigma1 : Sigma1 values for all radial bins\n",
    "        plot : bool. Whether to plot excess kurtosis vs radius or not\n",
    "\n",
    "    Returns:\n",
    "        r_centers :  Bin centers\n",
    "        excess_kurtosis : Excess kurtosis values for each radial bin\n",
    "    \"\"\"\n",
    "    # bin centers\n",
    "    r_centers = np.array([(r_min + r_max) / 2 for r_min, r_max in r_bins])\n",
    "    lambdas = np.array(lambdas)\n",
    "\n",
    "    # Variance and 4th moment of model\n",
    "    var_mix = lambdas * sigma1**2 + (1 - lambdas) * sigma**2\n",
    "    m4_mix = lambdas * 3 * sigma1**4 + (1 - lambdas) * 3 * sigma**4\n",
    "    excess_kurtosis = m4_mix / var_mix**2 \n",
    "\n",
    "    if plot==True:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(r_centers, excess_kurtosis, 'o-', color='navy')\n",
    "\n",
    "        plt.xlabel(\"Radius $r$ \", size=14)\n",
    "        plt.ylabel(\" Kurtosis\",size=14)\n",
    " \n",
    "        plt.tick_params(axis='both', which='major',length=6, width=2,labelsize=14) \n",
    "        plt.grid(True, linestyle=':')\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig(\"Kurtosis_1halo.pdf\", dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    return r_centers, excess_kurtosis\n",
    "\n",
    "kurtosis_vs_radius(r_bins,results[:,2], results[:,0], results[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65350fb",
   "metadata": {},
   "source": [
    "So many codeblocks are inefficient or chunky haha, can definitely be improved!! I would suggest trying MCMC fitting routines instead of minimize :))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ec92d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
